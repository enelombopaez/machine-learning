{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Casos prácticos\n",
    "\n",
    "En este notebook vamos a abordar dos casos prácticos interesantes.\n",
    "\n",
    "1. Predicción (*forecasting*) de la demanda de bicicletas\n",
    "2. Clasificación multiclase de imágenes\n",
    "\n",
    "Lo primero es cargar las librerías y funciones necesarias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plot_confusion_matrix\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Predicción de la demanda de bicicletas\n",
    "\n",
    "El problema está descrito [aquí](https://christophm.github.io/interpretable-ml-book/bike-data.html), y los datos pueden descargarse en la [UCI Machine Learning Repository](http://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset), aunque los hemos modificado un poco para hacer el problema más parecido al [*challenge*](https://www.kaggle.com/c/bike-sharing-demand/data) original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./data/bikes.csv',sep=';', decimal='.')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\">\n",
    "EJERCICIO 9.1: Interpreta los datos y realiza tus primeras hipótesis sobre qué variables son de interés\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\">\n",
    "EJERCICIO 9.2: Elimina las columnas *instant*, *casual* y *registered*\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... código aquí\n",
    "data = ...\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 *Feature Engineering*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a trabajar con las fechas para crear algunas variables auxiliares. Podemos hacer esto con cualquier cadena de texto que represente una fecha, mediante la función `strptime` de Python y el ya conocido `apply` de Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "data['dteday'] = data['dteday'].apply(lambda x: datetime.strptime(x,'%d-%m-%Y'))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['year'] = data['dteday'].apply(lambda x: x.year - 2011)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['month'] = data['dteday'].apply(lambda x: x.month)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['weekday'] = data['dteday'].apply(lambda x: x.isoweekday())\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Llegados a este punto, podemos eliminar la variable *dteday*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(['dteday'], axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Análisis de correlación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Compute the correlation matrix\n",
    "corr = np.abs(data.drop(['cnt'], axis=1).corr())\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.zeros_like(corr, dtype=bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(12, 10))\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr, mask=mask,vmin = 0.0, vmax=1.0, center=0.5,\n",
    "            linewidths=.1, cmap=\"YlGnBu\", cbar_kws={\"shrink\": .8})\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\">\n",
    "EJERCICIO 9.3: Representa la variable *temp* vs *atemp*, por ejemplo mediante un scatter plot\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... código aquí"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A la vista está que son variables altamente correlacionadas. Podemos eliminar *temp*, ya que nos afecta más la sensación térmica que la temperatura real. O podemos eliminar *atemp*, si lo preferimos. Es una decisión del data scientist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(['temp'], axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podríamos seguir analizando de forma más exhaustiva, pero vamos a dejarlo aquí de momento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Codificación de variables categóricas\n",
    "\n",
    "Tenemos varias variables categóricas: *season*, *weathersit*, *month* y *weekday*. Cuando trabajamos con series temporales, es común crear variables *dummies* asociadas a cada una de las situaciones de las variables categóricas. Para ello, usamos get_dummies, que es básicamente el onehot de Pandas [pd.get_dummies()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html). Se aplica directamente sobre el dataframe puede [utilizarse](https://towardsdatascience.com/the-dummys-guide-to-creating-dummy-variables-f21faddb1d40) con strings directamente. La pega es que genera un nuevo dataframe que hay que agregar al original.\n",
    "\n",
    "La codificación \"dummy\" de variables categóricas en problemas de clasificación/regresión es en general una mala idea, como vimos anteriormente, porque aumenta mucho la dimensionalidad de entrada; en series temporales resulta adecuado para explicar el efecto de una situación temporal en la variable target.\n",
    "\n",
    "Comenzamos por la variable *season*. Veamos qué hace *get_dummies()*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies(data['season'], prefix = 'season')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = pd.get_dummies(data['season'], prefix = 'season')\n",
    "\n",
    "data = pd.concat([data,dummy],axis=1).drop(['season'],axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 División train/test \n",
    "\n",
    "Sí, sí, habría que hacerla antes. Pido perdón (:\n",
    "\n",
    "Vamos a ver cómo hacerla cuando tenemos datos con tendencia o estacionalidad, es decir, cuando hay componente temporal. No podemos aleatorizar alegremente, porque perderíamos ambas cosas; tenemos que escoger un punto a partir del cual consideraremos que los datos son de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparamos los datos\n",
    "features = data.columns.drop(['cnt'])\n",
    "X = data[features].values\n",
    "y = data['cnt'].values\n",
    "\n",
    "print('Filas, columnas', X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a escoger un offset que deje un 25% de los datos para test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 1:\n",
    "offset = 182 # 0.25 of 731\n",
    "\n",
    "X_train = X[:-offset, :]\n",
    "y_train = y[:-offset]\n",
    "X_test  = X[-offset:, :]\n",
    "y_test  = y[-offset:]\n",
    "\n",
    "plt.plot(range(0,len(y_train)),y_train, label='train')\n",
    "plt.plot(range(len(y_train),len(y)),y_test,label='test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Validación cruazada y búsqueda de parámetros libres\n",
    "\n",
    "De nuevo, hay diferencias. La validación cruzada se hace con un objeto del tipo [TimeSeriesSplit](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.TimeSeriesSplit.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=3)\n",
    "\n",
    "alpha_vector = np.logspace(-4,4,20)\n",
    "param_grid = {'alpha': alpha_vector}\n",
    "\n",
    "grid = GridSearchCV(Lasso(), param_grid=param_grid, cv = TimeSeriesSplit(n_splits=3).split(X_train)).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"best parameters: {}\".format(grid.best_params_))\n",
    "\n",
    "scores = grid.cv_results_['mean_test_score']\n",
    "std_scores = grid.cv_results_['std_test_score']\n",
    "plt.errorbar(np.log10(alpha_vector),scores,yerr=std_scores, fmt='o-',ecolor='g')\n",
    "plt.xlabel('log(alpha)',fontsize=16)\n",
    "plt.ylabel('CV MSE')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Métricas en test\n",
    "\n",
    "Finalmente, calculamos el rendimiento del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "alpha_optimo = grid.best_params_['alpha']\n",
    "lasso = Lasso(alpha = alpha_optimo).fit(X_train,y_train)\n",
    "\n",
    "ytrainLasso = lasso.predict(X_train)\n",
    "ytestLasso  = lasso.predict(X_test)\n",
    "\n",
    "mseTrainModelLasso = mean_squared_error(y_train,ytrainLasso)\n",
    "mseTestModelLasso = mean_squared_error(y_test,ytestLasso)\n",
    "\n",
    "print('MSE Modelo Lasso (train): %0.3g' % mseTrainModelLasso)\n",
    "print('MSE Modelo Lasso (test) : %0.3g' % mseTestModelLasso)\n",
    "\n",
    "w = lasso.coef_\n",
    "for f,wi in zip(features,w):\n",
    "    print(f,wi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\">\n",
    "EJERCICIO 9.4: Representa la predicción obtenida junto con la serie real (train+test)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(0,len(y_train)),y_train, label='train')\n",
    "plt.plot(range(len(y_train),len(y)),y_test,label='test')\n",
    "\n",
    "# ... código aquí\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Clasificación multiclase de imágenes\n",
    "\n",
    "En este caso vamos a utilizar la famosa base de datos de [MNIST](http://yann.lecun.com/exdb/mnist/). Esta base de datos contiene\n",
    "\n",
    "* Training set: train-images-idx3-ubyte.gz (9.9 MB, 47 MB unzipped, 60,000 samples)\n",
    "* Training set labels: train-labels-idx1-ubyte.gz (29 KB, 60 KB unzipped, 60,000 labels)\n",
    "* Test set images: t10k-images-idx3-ubyte.gz (1.6 MB, 7.8 MB, 10,000 samples)\n",
    "* Test set labels: t10k-labels-idx1-ubyte.gz (5 KB, 10 KB unzipped, 10,000 labels)\n",
    "\n",
    "Estas imágenes se pueden descargar a partir del siguiente código, asumiendo que tenéis los archivos (están en la carpeta `/data`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gzip\n",
    " \n",
    "def load_mnist(path, kind='train'):\n",
    "    \"\"\"Load MNIST data from `path`\"\"\"\n",
    "    labels_path = os.path.join(path, '%s-labels-idx1-ubyte.gz' % kind)\n",
    "    images_path = os.path.join(path, '%s-images-idx3-ubyte.gz' % kind)\n",
    "        \n",
    "    with gzip.open(labels_path, 'rb') as lbpath:\n",
    "        lbpath.read(8)\n",
    "        buffer = lbpath.read()\n",
    "        labels = np.frombuffer(buffer, dtype=np.uint8)\n",
    "\n",
    "    with gzip.open(images_path, 'rb') as imgpath:\n",
    "        imgpath.read(16)\n",
    "        buffer = imgpath.read()\n",
    "        images = np.frombuffer(buffer, dtype=np.uint8).reshape(len(labels), 784).astype(np.float64)\n",
    " \n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = load_mnist('./data/mnist/', kind='train')\n",
    "print('Rows: %d, columns: %d' % (X_train.shape[0], X_train.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = load_mnist('./data/mnist/', kind='t10k')\n",
    "print('Rows: %d, columns: %d' % (X_test.shape[0], X_test.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¡Fíjate que el conjunto de entrenamiento son los pixels de la imagen tal cual!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"X train shape: \", X_train.shape)\n",
    "print (\"y train shape: \", y_train.shape)\n",
    "print (\"X test shape: \",  X_test.shape)\n",
    "print (\"y test shape: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\">\n",
    "EJERCICIO 9.5: Entrena un modelo de regresión logística con C = 10 y calcula sus prestaciones en el conjunto de test. A lo mejor te resulta de utilidad revisar la [documentación](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# ... código aquí\n",
    "lr = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\">\n",
    "EJERCICIO 9.6: Calcula y representa la matriz de confusión, ¿qué conclusiones puedes sacar?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# ... código aquí\n",
    "y_pred_test = ...\n",
    "confmat = confusion_matrix(y_test, y_pred_test)\n",
    "\n",
    "plot_confusion_matrix(confmat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
